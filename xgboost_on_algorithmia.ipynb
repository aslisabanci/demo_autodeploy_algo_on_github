{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with XGBoost on Algorithmia\n",
    "\n",
    "With this notebook, we will be training an XGBoost model on Amazon's Musical Instrument Reviews dataset and be able to use this model to predict the sentiment of the given texts. If you would like to see the final product first, you can check out this algorithm in action at https://algorithmia.com/algorithms/asli/xgboost_basic_sentiment_analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will step by step: \n",
    "\n",
    "1. Load the training data\n",
    "\n",
    "2. Preprocess the data\n",
    "\n",
    "3. Setup an XGBoost model and do a mini hyperparameter search\n",
    "\n",
    "4. Fit the data on our model\n",
    "\n",
    "5. Get the predictions\n",
    "\n",
    "6. Check the accuracy\n",
    "\n",
    "7. Pickle the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -r notebook_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data\n",
    "Let's load our training data, take a look at a few rows and one of the review texts in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       reviewerID        asin  \\\n0  A2IBPI20UZIR0U  1384719342   \n1  A14VAT5EAX3D9S  1384719342   \n2  A195EZSQDW3E21  1384719342   \n3  A2C00NNG1ZQQG2  1384719342   \n4   A94QU4C90B1AX  1384719342   \n\n                                       reviewerName   helpful  \\\n0  cassandra tu \"Yeah, well, that's just like, u...    [0, 0]   \n1                                              Jake  [13, 14]   \n2                     Rick Bennette \"Rick Bennette\"    [1, 1]   \n3                         RustyBill \"Sunday Rocker\"    [0, 0]   \n4                                     SEAN MASLANKA    [0, 0]   \n\n                                          reviewText  overall  \\\n0  Not much to write about here, but it does exac...      5.0   \n1  The product does exactly as it should and is q...      5.0   \n2  The primary job of this device is to block the...      5.0   \n3  Nice windscreen protects my MXL mic and preven...      5.0   \n4  This pop filter is great. It looks and perform...      5.0   \n\n                                 summary  unixReviewTime   reviewTime  \n0                                   good      1393545600  02 28, 2014  \n1                                   Jake      1363392000  03 16, 2013  \n2                   It Does The Job Well      1377648000  08 28, 2013  \n3          GOOD WINDSCREEN FOR THE MONEY      1392336000  02 14, 2014  \n4  No more pops when I record my vocals.      1392940800  02 21, 2014  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewerID</th>\n      <th>asin</th>\n      <th>reviewerName</th>\n      <th>helpful</th>\n      <th>reviewText</th>\n      <th>overall</th>\n      <th>summary</th>\n      <th>unixReviewTime</th>\n      <th>reviewTime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A2IBPI20UZIR0U</td>\n      <td>1384719342</td>\n      <td>cassandra tu \"Yeah, well, that's just like, u...</td>\n      <td>[0, 0]</td>\n      <td>Not much to write about here, but it does exac...</td>\n      <td>5.0</td>\n      <td>good</td>\n      <td>1393545600</td>\n      <td>02 28, 2014</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A14VAT5EAX3D9S</td>\n      <td>1384719342</td>\n      <td>Jake</td>\n      <td>[13, 14]</td>\n      <td>The product does exactly as it should and is q...</td>\n      <td>5.0</td>\n      <td>Jake</td>\n      <td>1363392000</td>\n      <td>03 16, 2013</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A195EZSQDW3E21</td>\n      <td>1384719342</td>\n      <td>Rick Bennette \"Rick Bennette\"</td>\n      <td>[1, 1]</td>\n      <td>The primary job of this device is to block the...</td>\n      <td>5.0</td>\n      <td>It Does The Job Well</td>\n      <td>1377648000</td>\n      <td>08 28, 2013</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A2C00NNG1ZQQG2</td>\n      <td>1384719342</td>\n      <td>RustyBill \"Sunday Rocker\"</td>\n      <td>[0, 0]</td>\n      <td>Nice windscreen protects my MXL mic and preven...</td>\n      <td>5.0</td>\n      <td>GOOD WINDSCREEN FOR THE MONEY</td>\n      <td>1392336000</td>\n      <td>02 14, 2014</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A94QU4C90B1AX</td>\n      <td>1384719342</td>\n      <td>SEAN MASLANKA</td>\n      <td>[0, 0]</td>\n      <td>This pop filter is great. It looks and perform...</td>\n      <td>5.0</td>\n      <td>No more pops when I record my vocals.</td>\n      <td>1392940800</td>\n      <td>02 21, 2014</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/amazon_musical_reviews/Musical_instruments_reviews.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\""
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "data[\"reviewText\"].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Time to process our texts! Basically, we'll:\n",
    "- Remove the English stopwords\n",
    "- Remove punctuations\n",
    "- Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/aslisabanci/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def threshold_ratings(data):\n",
    "    def threshold_overall_rating(rating):\n",
    "        return 0 if int(rating)<=3 else 1\n",
    "    data[\"overall\"] = data[\"overall\"].apply(threshold_overall_rating)\n",
    "\n",
    "def remove_stopwords_punctuation(data):\n",
    "    data[\"review\"] = data[\"reviewText\"] + data[\"summary\"]\n",
    "\n",
    "    puncs = list(punctuation)\n",
    "    stops = stopwords.words(\"english\")\n",
    "\n",
    "    def remove_stopwords_in_str(input_str):\n",
    "        filtered = [char for char in str(input_str).split() if char not in stops]\n",
    "        return ' '.join(filtered)\n",
    "\n",
    "    def remove_punc_in_str(input_str):\n",
    "        filtered = [char for char in input_str if char not in puncs]\n",
    "        return ''.join(filtered)\n",
    "\n",
    "    def remove_stopwords_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_stopwords_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    def remove_punc_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_punc_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    data[\"review\"] = remove_stopwords_in_series(data[\"review\"].str.lower())\n",
    "    data[\"review\"] = remove_punc_in_series(data[\"review\"].str.lower())\n",
    "\n",
    "def drop_unused_colums(data):\n",
    "    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', \"reviewText\", \"summary\"], axis=1, inplace=True)\n",
    "\n",
    "def preprocess_reviews(data):\n",
    "    remove_stopwords_punctuation(data)\n",
    "    threshold_ratings(data)\n",
    "    drop_unused_colums(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   overall                                             review\n0        1  much write here exactly supposed to filters po...\n1        1  product exactly quite affordablei realized dou...\n2        1  primary job device block breath would otherwis...\n3        1  nice windscreen protects mxl mic prevents pops...\n4        1  pop filter great looks performs like studio fi...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>overall</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>much write here exactly supposed to filters po...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>product exactly quite affordablei realized dou...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>primary job device block breath would otherwis...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>nice windscreen protects mxl mic prevents pops...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>pop filter great looks performs like studio fi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "preprocess_reviews(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 42\n",
    "X = data[\"review\"]\n",
    "y = data[\"overall\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini randomized search\n",
    "Let's set up a very basic cross-validated randomized search over parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"max_depth\": range(9,12), \"min_child_weight\": range(5,8)}\n",
    "rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to vectorize, transform and fit\n",
    "Time to vectorize our data, transform it and then fit our model to it.\n",
    "To be able to feed the text data as numeric values to our model, we will first convert our texts into a matrix of token counts using a CountVectorizer. Then we will convert the count matrix to a normalized tf-idf (term-frequency times inverse document-frequency) representation. Using this transformer, we will be scaling down the impact of tokens that occur very frequently, because they convey less information to us. On the contrary, we will be scaling up the impact of the tokens that occur in a small fraction of the training data because they are more informative to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n                ('model',\n                 RandomizedSearchCV(estimator=XGBClassifier(base_score=None,\n                                                            booster=None,\n                                                            colsample_bylevel=None,\n                                                            colsample_bynode=None,\n                                                            colsample_bytree=None,\n                                                            gamma=None,\n                                                            gpu_id=None,\n                                                            importance_type='gain',\n                                                            interaction_constraints=None,\n                                                            learning_rate=None,\n                                                            max_delta_step=None,\n                                                            max_depth=None,\n                                                            min_child_weight=None,\n                                                            missing=nan,\n                                                            monotone_constraints=None,\n                                                            n_estimators=100,\n                                                            n_jobs=None,\n                                                            num_parallel_tree=None,\n                                                            random_state=None,\n                                                            reg_alpha=None,\n                                                            reg_lambda=None,\n                                                            scale_pos_weight=None,\n                                                            subsample=None,\n                                                            tree_method=None,\n                                                            validate_parameters=None,\n                                                            verbosity=None),\n                                    n_iter=1,\n                                    param_distributions={'max_depth': range(9, 12),\n                                                         'min_child_weight': range(5, 8)}))])"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "model  = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', rand_search_cv)\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model Accuracy: 89.14\n"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {round(acc * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Deployment via Github Actions\n",
    "TODO: Update the instructions\n",
    "- Import the Algorithmia Github Action config utility script\n",
    "- Get the model file path to be saved from the YAML file - no need to manually enter it here.\n",
    "- Get the algorithm name from the YAML file too.\n",
    "- If desired, overwrite the algorithm script and the dependency file from within your notebook too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\"\"\"\n",
    "When using a locally embedded Github action, this should point to the src path of the Github action itself. The suggested best practice is to place the Github action folder under a .github/actions directory under the root level of your model development repository\n",
    "An example usage would be:\n",
    "sys.path.append(\".github/actions/algorithmia_ci_modeldeployment/src\")\n",
    "\n",
    "If you're using a Github action on another repository, it should point to /src, as this is where our Dockerized Github action copied its source files. So our path append snippet should be:\n",
    "sys.path.append(\"/src\")\n",
    "\"\"\"\n",
    "# Our example is using a Github action from another repo, so we append the /src dir to our path.\n",
    "sys.path.append(\"/src\")\n",
    "\n",
    "from action_config_utils import ActionConfigUtils\n",
    "config_utils = ActionConfigUtils()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "./autodeployed_model.pkl\nxgboost_automated\n"
    }
   ],
   "source": [
    "# Geting the Algorithmia algo name and the to-be-deployed model file name from the Github action config file, in order to reduce duplication. This notebook will:\n",
    "# 1. Save the created model object to a local path. So we're extracting the configured file path from our Github action yaml. The Github action will then take the model object from this path and upload it to Algorithmia.\n",
    "modelfile_relativepath = config_utils.get_model_relativepath(default_path=\"./autodeployed_model.pkl\")\n",
    "print(modelfile_relativepath)\n",
    "\n",
    "# 2. Update its algorithm and dependency files on Algorithmia, with the custom code written on this notebook. So we're extracting the configured algorithm name from our Github action yaml. The Github action will do a Git add + commit + push on these files and trigger a new build of the algorithm on Algorithmia.\n",
    "algo_name = config_utils.get_algoname(default_name=\"xgboost_automated\")\n",
    "print(algo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['./autodeployed_model.pkl']"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "joblib.dump(model, modelfile_relativepath, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_script_path, algo_requirements_path = config_utils.get_algorithmia_filepaths(algo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Writing $algo_script_path\n"
    }
   ],
   "source": [
    "%%writefile $algo_script_path\n",
    "import Algorithmia\n",
    "import json\n",
    "import os.path\n",
    "import joblib\n",
    "import xgboost\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "#I'm generated via a notebook and pushed via Github Actions!\n",
    "\n",
    "client = Algorithmia.client()\n",
    "\n",
    "def load_model_config(config_rel_path=\"../model_config.json\"):\n",
    "    \"\"\"Loads the model manifest file as a dict. \n",
    "    A manifest file has the following structure:\n",
    "    {\n",
    "      \"model_filepath\": Uploaded model path on Algorithmia data collection\n",
    "      \"model_md5_hash\": MD5 hash of the uploaded model file\n",
    "      \"model_origin_repo\": Model development repository having the Github CI workflow\n",
    "      \"model_origin_commit_SHA\": Commit SHA related to the trigger of the CI workflow\n",
    "      \"model_origin_commit_msg\": Commit message related to the trigger of the CI workflow\n",
    "      \"model_uploaded_utc\": UTC timestamp of the automated model upload\n",
    "    }\n",
    "    \"\"\"\n",
    "    config = []\n",
    "    config_path = \"{}/{}\".format(os.path.dirname(__file__), (config_rel_path))\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path) as json_file:\n",
    "            config = json.load(json_file)\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_model(config):\n",
    "    \"\"\"Loads the model object from the file at model_filepath key in config dict\"\"\"\n",
    "    model_path = config[\"model_filepath\"]\n",
    "    model_file = client.file(model_path).getFile().name\n",
    "    model_obj = joblib.load(model_file)\n",
    "    return model_file, model_obj\n",
    "\n",
    "def assert_model_md5(model_file):\n",
    "    \"\"\"\n",
    "    Calculates the loaded model file's MD5 and compares the actual file hash with the hash on the model manifest\n",
    "    \"\"\"\n",
    "    md5_hash = None\n",
    "    DIGEST_BLOCK_SIZE = 128 * 64\n",
    "    with open(model_file, \"rb\") as f:\n",
    "        md5_hasher = hashlib.md5()\n",
    "        buf = f.read(DIGEST_BLOCK_SIZE)\n",
    "        while len(buf) > 0:\n",
    "            md5_hasher.update(buf)\n",
    "            buf = afile.read(DIGEST_BLOCK_SIZE)\n",
    "        md5_hash = file_hash.hexdigest()\n",
    "    assert config[\"model_md5_hash\"] == md5_hash\n",
    "    \n",
    "def assert_model_pipeline_steps(model_obj):\n",
    "    \"\"\"For demonstration purposes, asserts that the XGBoost model has the expected pipeline steps.\n",
    "    \"\"\"\n",
    "    assert xgb.steps[0][0] == \"vect\"\n",
    "    assert xgb.steps[1][0] == \"tfidf\"\n",
    "    assert xgb.steps[2][0] == \"model\"\n",
    "    print(\"All assertions are okay, we have a perfectly uploaded model!\")\n",
    "\n",
    "\n",
    "config = load_model_config()\n",
    "xgb_path, xgb_obj = load_model(config)\n",
    "assert_model_md5(xgb_path)\n",
    "assert_model_pipeline_steps(xgb_obj)\n",
    "\n",
    "# API calls will begin at the apply() method, with the request body passed as 'input'\n",
    "# For more details, see algorithmia.com/developers/algorithm-development/languages\n",
    "def apply(input):\n",
    "    series_input = pd.Series([input])\n",
    "    result = xgb.predict(series_input)\n",
    "    return {\n",
    "        \"sentiment\": result.tolist()[0], \n",
    "        \"predicting_model_metadata\": {\n",
    "            \"model_file\": config[\"model_filepath\"],\n",
    "            \"origin_repo\": config[\"model_origin_repo\"], \n",
    "            \"origin_commit_SHA\": config[\"model_origin_commit_SHA\"], \n",
    "            \"origin_commit_msg\": config[\"model_origin_commit_msg\"]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $algo_requirements_path\n",
    "# I'm generated via a notebook and pushed via Github Actions!\n",
    "algorithmia>=1.0.0,<2.0\n",
    "scikit-learn\n",
    "pandas\n",
    "numpy\n",
    "joblib\n",
    "xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}