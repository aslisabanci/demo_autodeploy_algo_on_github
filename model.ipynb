{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with XGBoost on Algorithmia\n",
    "\n",
    "With this notebook, we will be training an XGBoost model on Amazon's Musical Instrument Reviews dataset and be able to use this model to predict the sentiment of the given texts. If you would like to see the final product first, you can check out this algorithm in action at https://algorithmia.com/algorithms/asli/xgboost_basic_sentiment_analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will step by step: \n",
    "\n",
    "1. Load the training data\n",
    "\n",
    "2. Preprocess the data\n",
    "\n",
    "3. Setup an XGBoost model and do a mini hyperparameter search\n",
    "\n",
    "4. Fit the data on our model\n",
    "\n",
    "5. Get the predictions\n",
    "\n",
    "6. Check the accuracy\n",
    "\n",
    "7. Pickle the final model\n",
    "\n",
    "8. Create the inference script and the dependency file of our algorithm on Algorithmia\n",
    "\n",
    "To get our model file and the inference scripts uploaded to Algorithmia and get linked together, we will be persisting them on paths that we'll read from our Github Actions workflow YAML file. Our utility functions will be able to retrieve that information when this notebook is executed on Github's workflow machines, so no need to worry about internals!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data\n",
    "Let's load our training data, take a look at a few rows and one of the review texts in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/amazon_musical_reviews/Musical_instruments_reviews.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"reviewText\"].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Time to process our texts! Basically, we'll:\n",
    "- Remove the English stopwords\n",
    "- Remove punctuations\n",
    "- Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def threshold_ratings(data):\n",
    "    def threshold_overall_rating(rating):\n",
    "        return 0 if int(rating)<=3 else 1\n",
    "    data[\"overall\"] = data[\"overall\"].apply(threshold_overall_rating)\n",
    "\n",
    "def remove_stopwords_punctuation(data):\n",
    "    data[\"review\"] = data[\"reviewText\"] + data[\"summary\"]\n",
    "\n",
    "    puncs = list(punctuation)\n",
    "    stops = stopwords.words(\"english\")\n",
    "\n",
    "    def remove_stopwords_in_str(input_str):\n",
    "        filtered = [char for char in str(input_str).split() if char not in stops]\n",
    "        return ' '.join(filtered)\n",
    "\n",
    "    def remove_punc_in_str(input_str):\n",
    "        filtered = [char for char in input_str if char not in puncs]\n",
    "        return ''.join(filtered)\n",
    "\n",
    "    def remove_stopwords_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_stopwords_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    def remove_punc_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_punc_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    data[\"review\"] = remove_stopwords_in_series(data[\"review\"].str.lower())\n",
    "    data[\"review\"] = remove_punc_in_series(data[\"review\"].str.lower())\n",
    "\n",
    "def drop_unused_colums(data):\n",
    "    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', \"reviewText\", \"summary\"], axis=1, inplace=True)\n",
    "\n",
    "def preprocess_reviews(data):\n",
    "    remove_stopwords_punctuation(data)\n",
    "    threshold_ratings(data)\n",
    "    drop_unused_colums(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocess_reviews(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 42\n",
    "X = data[\"review\"]\n",
    "y = data[\"overall\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini randomized search\n",
    "Let's set up a very basic cross-validated randomized search over parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"max_depth\": range(9,12), \"min_child_weight\": range(5,8)}\n",
    "rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to vectorize, transform and fit\n",
    "Time to vectorize our data, transform it and then fit our model to it.\n",
    "To be able to feed the text data as numeric values to our model, we will first convert our texts into a matrix of token counts using a CountVectorizer. Then we will convert the count matrix to a normalized tf-idf (term-frequency times inverse document-frequency) representation. Using this transformer, we will be scaling down the impact of tokens that occur very frequently, because they convey less information to us. On the contrary, we will be scaling up the impact of the tokens that occur in a small fraction of the training data because they are more informative to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model  = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', rand_search_cv)\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {round(acc * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Deployment via Github Actions\n",
    "TODO: Update the instructions further\n",
    "- Import the Algorithmia Github Action config utility script\n",
    "- Get the model file path to be saved from the YAML file - no need to manually enter it here.\n",
    "- Get the algorithm name from the YAML file too.\n",
    "- If desired, overwrite the algorithm script and the dependency file from within your notebook too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should save the created model object to a local path. The Github action will then take the model object from this path and upload it to Algorithmia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, \"model.pkl\", compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing inference code (the algorithm)\n",
    "\n",
    "We will convert our inference (Algorithm) notebook named `xgboost_automated.ipynb` into a Python script named `xgboost_automated.py` - with the same name as our algorithm. \n",
    "\n",
    "And this is exactly what the Github Action would do if it doesn't find a `xgboost_automated.py` file in our repository. In that case, it would make this conversion for us and then push this file to the algorithm `src` directory at Algorithmia.\n",
    "\n",
    "After the conversion, we will test that it works by simply executing this file with the `python3` executable. The script will begin its execution through our `if __name__ == \"__main__\"` line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[NbConvertApp] Converting notebook demo_algo.ipynb to script\n[NbConvertApp] Writing 3362 bytes to demo_algo.py\nModel file MD5 assertion done.\nModel object pipeline steps assertion done\nAll assertions are okay, we have a perfectly uploaded model!\n'Calling algorithm apply() func with input: This guitar is great!'\n{'predicting_model_metadata': {'model_file': 'asli/xgboost_automated/autodeployed_model_348410b2f7071377de060f030cca7d984b0f2dca.pkl',\n                               'origin_commit_SHA': '348410b2f7071377de060f030cca7d984b0f2dca',\n                               'origin_commit_msg': 'Reverted multiple '\n                                                    'workflow testing',\n                               'origin_repo': 'aslisabanci/demo_autodeploy_algo_on_algorithmia'},\n 'sentiment': 1}\n"
    }
   ],
   "source": [
    "# !jupyter nbconvert --to script xgboost_automated.ipynb\n",
    "# !python3 xgboost_automated.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final checks before committing this notebook\n",
    "\n",
    "- Make sure `xgboost_automated.ipynb` or `xgboost_automated.py` files are in good shape to be pushed to Algorithmia. Remember that if you check-in xgboost_automated.py in this repository, Github Action will take this file instead of converting `xgboost_automated.ipynb` to a `.py` file.\n",
    "\n",
    "- Make sure Algorithmia API Key is not committed and pushed in any file!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}